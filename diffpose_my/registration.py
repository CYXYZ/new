# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/api/03_registration.ipynb.

# %% auto 0
__all__ = ['PoseRegressor', 'SparseRegistration', 'VectorizedNormalizedCrossCorrelation2d']

# %% ../notebooks/api/03_registration.ipynb 3
import timm
import torch

# %% ../notebooks/api/03_registration.ipynb 5
from diffdrr.pose import RigidTransform, convert


class PoseRegressor(torch.nn.Module):
    """
    A PoseRegressor is comprised of a pretrained backbone model that extracts features
    from an input X-ray and two linear layers that decode these features into rotational
    and translational camera pose parameters, respectively.
    """

    def __init__(
        self,
        model_name,
        parameterization,
        convention=None,
        pretrained=False,
        **kwargs,
    ):
        super().__init__()

        self.parameterization = parameterization
        self.convention = convention
        n_angular_components = N_ANGULAR_COMPONENTS[parameterization]

        # Get the size of the output from the backbone
        self.backbone = timm.create_model(
            model_name,
            pretrained,
            num_classes=0,
            in_chans=1,
            **kwargs,
        )
        output = self.backbone(torch.randn(1, 1, 256, 256)).shape[-1]
        self.xyz_regression = torch.nn.Linear(output, 3)
        self.rot_regression = torch.nn.Linear(output, n_angular_components)
        # self.xyz_regression_with_input = torch.nn.Linear(output, 4)
        # self.rot_regression_with_input = torch.nn.Linear(output, n_angular_components+1)

    def forward(self, x):
        x = self.backbone(x)
        rot = self.rot_regression(x)
        # print('rot1',rot)
        # rot = self.rot_regression_with_input(x)[:, :3]
        # print('\nrot2',rot)

        xyz = self.xyz_regression(x)
        # print('\nxyz1',xyz)
        # xyz = self.xyz_regression_with_input(x)[:, :3]
        # print('\nxyz2',xyz)




        # print('self.parameterization',self.parameterization)
        # print("covert",convert(
        #     rot,
        #     xyz,
        #     parameterization=self.parameterization,
        #     convention=self.convention,
        # ))
        # print('\ncovnert',convert(
        #     rot,
        #     xyz,
        #     parameterization=self.parameterization,
        #     convention=self.convention,
        # ).matrix)

        return convert(
            rot,
            xyz,
            parameterization=self.parameterization,
            convention=self.convention,
        )

# %% ../notebooks/api/03_registration.ipynb 6
N_ANGULAR_COMPONENTS = {
    "axis_angle": 3,
    "euler_angles": 3,
    "se3_log_map": 3,
    "quaternion": 4,
    "rotation_6d": 6,
    "rotation_10d": 10,
    "quaternion_adjugate": 10,
}

# %% ../notebooks/api/03_registration.ipynb 11
from diffdrr.drr import DRR
from diffdrr.detector import Detector
from diffdrr.renderers import Siddon

class SparseRegistration(torch.nn.Module):

    """稀疏配准模块，用于在稀疏渲染条件下配准两个图像。

    Args:
        drr (DRR): 数字重建放射图对象。
        pose (RigidTransform): 刚体变换对象，表示姿态。
        parameterization (str): 姿态参数化方式。
        convention (str, optional): 旋转矩阵的约定方式，默认为 None。
        features (Tensor, optional): 用于计算mNCC的偏差估计的特征，默认为 None。
        n_patches (int, optional): 稀疏渲染的块数，默认为 None。
        patch_size (int, optional): 块的大小，默认为 13。
    """

    def __init__(
        self,
        drr: DRR,
        pose: RigidTransform,
        parameterization: str,
        convention: str = None,
        features=None,  # # 用于计算mNCC的偏差估计的特征
        n_patches: int = None,  # 稀疏渲染的块数
        patch_size: int = 13,  # 块的大小
    ):
        super().__init__()
        self.drr = drr

        # 解析输入姿态
        rotation, translation = pose.convert(parameterization,convention)
        #改了的
        
        self.parameterization = parameterization
        self.convention = convention
        # 构建的参数
        self.rotation = torch.nn.Parameter(rotation)
        self.translation = torch.nn.Parameter(translation)

        # 计算剪裁边缘像素，以确保像素不会落在图像外部
        self.n_patches = n_patches
        self.patch_size = patch_size
        self.patch_radius = self.patch_size // 2 + 1
        self.height = self.drr.detector.height
        self.width = self.drr.detector.width
        self.f_height = self.height - 2 * self.patch_radius
        self.f_width = self.width - 2 * self.patch_radius

        # 定义块中心的分布
        if features is None:
            features = torch.ones(
                self.height, self.width, device=self.rotation.device
            ) / (self.height * self.width)
        self.patch_centers = torch.distributions.categorical.Categorical(
            probs=features.squeeze()[
                self.patch_radius : -self.patch_radius,
                self.patch_radius : -self.patch_radius,
            ].flatten()
        )

    def forward(self, n_patches=None, patch_size=None):
        # 解析初始密度
        if not hasattr(self.drr, "density"):
            self.drr.set_bone_attenuation_multiplier(
                self.drr.bone_attenuation_multiplier
            )

        if n_patches is not None or patch_size is not None:
            self.n_patches = n_patches
            self.patch_size = patch_size

        # 创建稀疏渲染的掩码
        if self.n_patches is None:
            mask = torch.ones(
                1,
                self.height,
                self.width,
                dtype=torch.bool,
                device=self.rotation.device,
            )
        else:
            mask = torch.zeros(
                self.n_patches,
                self.height,
                self.width,
                dtype=torch.bool,
                device=self.rotation.device,
            )
            radius = self.patch_size // 2
            idxs = self.patch_centers.sample(sample_shape=torch.Size([self.n_patches]))
            idxs, jdxs = (
                idxs // self.f_height + self.patch_radius,
                idxs % self.f_width + self.patch_radius,
            )

            idx = torch.arange(-radius, radius + 1, device=self.rotation.device)
            patches = torch.cartesian_prod(idx, idx).expand(self.n_patches, -1, -1)
            patches = patches + torch.stack([idxs, jdxs], dim=-1).unsqueeze(1)
            patches = torch.concat(
                [
                    torch.arange(self.n_patches, device=self.rotation.device)
                    .unsqueeze(-1)
                    .expand(-1, self.patch_size**2)
                    .unsqueeze(-1),
                    patches,
                ],
                dim=-1,
            )
            mask[
                patches[..., 0],
                patches[..., 1],
                patches[..., 2],
            ] = True

        # 转换姿态为相机坐标系
        pose = convert(self.rotation, 
                       self.translation,
                       parameterization=self.parameterization,
                       convention=self.convention,)

        source, target = self.drr.detector.forward(pose)

        # 渲染稀疏图像
        target = target[mask.any(dim=0).view(1, -1)]

        siddon = Siddon(eps=1e-8)
        img = siddon(self.drr.density, self.drr.spacing, source, target)
        
        if self.n_patches is None:
            img = self.drr.reshape_transform(img, batch_size=len(self.rotation))
        return img, mask

    def get_current_pose(self):

        # 自己改的一点
        return convert(self.rotation, 
                       self.translation,
                       parameterization=self.parameterization,
                       convention=self.convention,)

# %% ../notebooks/api/03_registration.ipynb 13
def preprocess(x, eps=1e-4):
    x = (x - x.min()) / (x.max() - x.min() + eps)
    return (x - 0.3080) / 0.1494


def pred_to_patches(pred_img, mask, n_patches, patch_size):
    return pred_img.expand(-1, n_patches, -1)[..., mask[..., mask.any(dim=0)]].reshape(
        1, n_patches, -1
    )


def img_to_patches(img, mask, n_patches, patch_size):
    return img.expand(-1, n_patches, -1, -1)[..., mask].reshape(1, n_patches, -1)


def mask_to_img(img, mask):
    return img[..., mask.any(dim=0)]


def vector_to_img(pred_img, mask):
    patches = [pred_img]
    filled = torch.zeros(1, 1, *mask[0].shape, device=pred_img.device)
    filled[...] = torch.nan
    for idx in range(len(mask)):
        patch = pred_img[:, mask[idx][mask.any(dim=0)]]
        filled[..., mask[idx]] = patch
        patches.append(patch)
    return filled

# %% ../notebooks/api/03_registration.ipynb 14
class VectorizedNormalizedCrossCorrelation2d(torch.nn.Module):
    def __init__(self, eps=1e-4):
        super().__init__()
        self.eps = eps

    def forward(self, img, pred_img, mask, n_patches, patch_size):
        pred_img = preprocess(pred_img).unsqueeze(0)
        sub_img = mask_to_img(img, mask)
        pred_patches = pred_to_patches(pred_img, mask, n_patches, patch_size)
        img_patches = img_to_patches(img, mask, n_patches, patch_size)

        local_ncc = self.forward_compute(pred_patches, img_patches)
        global_ncc = self.forward_compute(pred_img, sub_img)
        return (local_ncc + global_ncc) / 2

    def forward_compute(self, x1, x2):
        assert x1.shape == x2.shape, "Input images must be the same size"
        x1, x2 = self.norm(x1), self.norm(x2)
        ncc = (x1 * x2).mean(dim=[-1, -2])
        return ncc

    def norm(self, x):
        mu = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, correction=0) + self.eps
        std = var.sqrt()
        return (x - mu) / std